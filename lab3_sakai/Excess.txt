# Code sourced from: https://www.geeksforgeeks.org/twitter-sentiment-analysis-using-python/
# Clean
def clean_tweet(tweet): 
    ''' 
    Utility function to clean tweet text by removing links, special characters 
    using simple regex statements. 
    '''
    return ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t]) |(\w+:\/\/\S+)", " ", tweet).split()) 

# Create an empty list for storage
temp_list = []
i = 0 # Initialize a counter
while i < len(tweets): # Iterate over the list of tweets
    try: # Not every tweet has a 'text' field in it so first we'll try the above cleaner function
        temp_list.append(clean_tweet(tweets[i]['text']))
    except: # If not, the loop will pass over it
        pass
    i += 1 # Increment the counter

# Code sourced from: https://stackoverflow.com/questions/20078816/replace-non-ascii-characters-with-a-single-space
# Create an empty list for storage
clean_list = []
i = 0 # Initialize a counter
while i < len(temp_list): # Iterate over a list
    clean_list.append(re.sub(r'[^\x00-\x7F]+',' ', temp_list[i])) # Remove all non-ASCII characters
    i += 1 # Increment the counter
    
# Display the list
clean_list


# Create an empty list for storage
analysis_list = [] 

# Create a list of ten tweets for NLP analysis
analysis_list.append([clean_list[63], clean_list[113], clean_list[248], clean_list[340], clean_list[355], clean_list[382],
                     clean_list[711], clean_list[766], clean_list[937], clean_list[944]])

# Flatten list
analysis_list = [item for sublist in analysis_list for item in sublist]

# Code sourced from: https://stackoverflow.com/questions/21546739/load-data-from-txt-with-pandas
# Read in score using a delimiter of '\t'
AFINN_df = pd.read_csv('AFINN-111.txt', delimiter="\t", header = None, encoding = 'utf-8')

# Rename column headers
AFINN_df.columns = ['word', 'score']

analysis_list